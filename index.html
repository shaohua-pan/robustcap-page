<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">

  <meta property="og:title"
    content="Fusing Monocular Images and Sparse IMU Signals for Real-time Human Motion Capture" />
  <meta property="og:url" content="https://shaohua-pan.github.io/robustcap-page/" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <title>Fusing Monocular Images and Sparse IMU Signals for Real-time Human Motion Capture</title>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-EWRYSDM25P"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-EWRYSDM25P');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="icon" href="static/figures/icon2.png">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://shaohua-pan.github.io">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>

        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Research
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://shaohua-pan.github.io/robustcap-page">
              RobustCap
            </a>
          </div>
        </div>
      </div>

    </div>
  </nav>

  <section class="publication-header">
    <div class="hero-body">
      <div class="container is-max-widescreen">
        <!-- <div class="columns is-centered"> -->
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Fusing Monocular Images and Sparse IMU Signals for Real-time Human
            Motion Capture</h1>
          <div class="is-size-3 publication-authors">
            SIGGRAPH ASIA 2023
          </div>
        </div>
      </div>
    </div>
  </section>
  <section class="publication-author-block">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://shaohua-pan.github.io">Shaohua Pan</a><sup>1</sup>,</span>
              <span class="author-block">
                Qi Ma<sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://xinyu-yi.github.io/">Xinyu Yi</a><sup>1</sup>,
              </span>
              <span class="author-block">
                Weifeng Hu<sup>2</sup>,
              </span>
              <span class="author-block">
                Xiong Wang<sup>2</sup>,
              </span>
              <span class="author-block">
                Xingkang ZHOU<sup>2</sup>,
              </span>
              <span class="author-block">
                Jijunnan LI<sup>2</sup>
              </span>
              <span class="author-block">
                <a href="http://xufeng.site/">Feng Xu</a><sup>1</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Tsinghua University,</span>
              <span class="author-block"><sup>2</sup>OPPO Research Institute</span>
            </div>


            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="" target="_blank" class="external-link button is-normal is-rounded">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="https://github.com/shaohua-pan/RobustCap" target="_blank"
                    class="external-link button is-normal is-rounded">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>

                </span>

                <span class="link-block">
                  <a href="" target="_blank" class="external-link button is-normal is-rounded">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span>
              </div>
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="item">
            <p style="margin-bottom: 30px">
              <iframe width="720" height="405" src="https://www.youtube.com/embed/URxUtzkF5Uk"
                title="YouTube video player" frameborder="0"
                allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                allowfullscreen></iframe>
            </p>
          </div>
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Either RGB images or inertial signals have been used for the task of motion capture (mocap), but combining
              them together is a new and interesting topic.
              We believe that the combination is complementary and able to solve the inherent difficulties of using one
              modality input, including occlusions, extreme lighting/texture, and out-of-view for visual mocap and
              global drifts for inertial mocap.
              To this end, we propose a method that fuses monocular images and sparse IMUs for real-time human motion
              capture.
              Our method contains a dual coordinate strategy to fully explore the IMU signals with different goals in
              motion capture.
              To be specific, besides one branch transforming the IMU signals to the camera coordinate system to combine
              with the image information, there is another branch to learn from the IMU signals in the body root
              coordinate system to better estimate body poses.
              Furthermore, a hidden state feedback mechanism is proposed for both two branches to compensate for their
              own drawbacks in extreme input cases.
              Thus our method can easily switch between the two kinds of signals or combine them in different cases to
              achieve a robust mocap.
              %The two divided parts can help each other for better mocap results under different conditions.
              Quantitative and qualitative results demonstrate that by delicately designing the fusion method, our
              technique significantly outperforms the state-of-the-art vision, IMU, and combined methods on both global
              orientation and local pose estimation.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>



  <section class="hero is-light">
    <div class="hero-body">
      <div class="container">

        <div class="column is-centered has-text-centered">
          <img src="static/images/method.jpg" width="720" />
        </div>


      </div>
    </div>
    </div>
  </section>

  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Live Demo</h2>
          <p>
            Our system can reconstruct human motion from monocular videos and sparse IMU signals in real-time.
          </p>
        </div>
      </div>
    </div>
    </div>
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="column is-centered has-text-centered">
            <p><b>
                <font size="6">Dark environment</font>
              </b></p>
            <video poster="" id="tree" autoplay controls muted loop height="100%">
              <source src="static/videos/dark.mp4" type="video/mp4">
            </video>
          </div>
          <div class="column is-centered has-text-centered">
            <p><b>
                <font size="6">Occlusion</font>
              </b></p>
            <video poster="" id="tree" autoplay controls muted loop height="100%">
              <source src="static/videos/occlusion.mp4" type="video/mp4">
            </video>
          </div>
          <div class="column is-centered has-text-centered">
            <p><b>
                <font size="6">Challenging motion</font>
              </b></p>
            <video poster="" id="tree" autoplay controls muted loop height="100%">
              <source src="static/videos/challenging.mp4" type="video/mp4">
            </video>
          </div>
          <div class="column is-centered has-text-centered">
            <p><b>
                <font size="6">Out of camera view
              </b></font>
            </p>
            <video poster="" id="tree" autoplay controls muted loop height="100%">
              <source src="static/videos/out_of_camera.mp4" type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div>
  </section>


  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Comparison</h2>
          <p>
            Our method superior vision-based methods (ROMP, PARE), IMU-based methods (PIP, TIP).
          </p>
        </div>
      </div>
    </div>
    </div>
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="column is-centered has-text-centered">
            <p><b>
                <font size="6">Challenging motion</font>
              </b></p>
            <video poster="" id="tree" autoplay controls muted loop height="100%">
              <source src="static/videos/c_comp.mp4" type="video/mp4">
            </video>
          </div>
          <div class="column is-centered has-text-centered">
            <p><b>
                <font size="6">Occlusion</font>
              </b></p>
            <video poster="" id="tree" autoplay controls muted loop height="100%">
              <source src="static/videos/occlusion_comp.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section hero is-light" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{EgoLocate2023,
        author = {Yi, Xinyu and Zhou, Yuxiao and Habermann, Marc and Golyanik, Vladislav and Pan, Shaohua and Theobalt, Christian and Xu, Feng},
        title = {EgoLocate: Real-time Motion Capture, Localization, and Mapping with Sparse Body-mounted Sensors},
        journal={ACM Transactions on Graphics (TOG)},
        year = {2023},
        volume = {42},
        number = {4},
        numpages = {17},
        articleno = {76},
        publisher = {ACM}
    }</code></pre>
    </div>
  </section>




  <footer class="footer">
    <div class="columns is-centered">
      <div class="column is-centered has-text-centered">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
              href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website source code based on the <a href="https://nerfies.github.io/"> Nerfies</a> and <a
              href="https://guytevet.github.io/mdm-page/"></a> MDM project pages. If you want
            to reuse their source code</a>, please credit them appropriately.
          </p>
        </div>
      </div>
    </div>
    </div>
  </footer>


  <script type="text/javascript">
    var sc_project = 12351448;
    var sc_invisible = 1;
    var sc_security = "c676de4f"; 
  </script>
  <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
  <noscript>
    <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
          class="statcounter" src="https://c.statcounter.com/12351448/0/c676de4f/1/" alt="Web Analytics"></a></div>
  </noscript>
  <!-- End of Statcounter Code -->

</body>

</html>